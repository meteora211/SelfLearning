# What Every Programmer Should Know about Memory

## 简介
古早的计算机设计比较简单，系统的多种组件，例如CPU，内存，网络接口等都是在一起开发的，因此这些组件的性能也比较平衡，例如内存和网络访问在数据访问上并不会比CPU快。

这种状况在计算机基本结构完善，以及硬件开发者关注独立子系统的性能之后得到了改变，一些子系统的性能大幅落后于其他组件并成为了性能瓶颈。
这一点在大容量存储以及内存上尤为突出。

大容量存储访问缓慢的问题主要通过软件技术来解决：操作系统会在主存上
常驻那些最常被访问的数据，而主存会比直接访问存储会快很多。缓存也被添加到存储系统
上来进一步的提升性能并不需要任何操作系统上的改动。本篇文章并不会对软件层面上的存储
性能优化做过多的探讨。

不同于存储系统，避免主存成为性能瓶颈更加具有挑战性，而且几乎所有的解决方案
都需要在硬件层面上做改动。如今这些改动主要以如下几种形式：

- RAM硬件设计（速度和并行层面）
- 内存控制器设计
- CPU缓存
- 设备直接内存访问（DMA）

这篇文章会主要探讨CPU缓存一级一些内存控制器设计的影响。
在探索这些主题的过程中，我们会从更上层的视角介绍DMA。
然而我们会先从如今商业化的硬件设计开始叹气，这是理解
问题和内存系统限制的前置条件。我们同样也会在不同程度
上了解不同类型的RAM并解释这些区别存在的原因。

这篇文章不会是一成不变的，而受限于商业硬件。当然我们会讨论更多
的相关主题，这些细节内容推荐读者们自行了解更为详细的文档。

对于操作系统特定的细节和方案，文章仅仅描述Linux系统，并不会
对其他操作系统做涉及。

在正式开始之前，文章包含大量的"一般化"以及类似的描述。文章描述
的技术仅仅是针对在现实世界中最为平常，主流使用的情景。而绝对化的
描述并不适用于这些技术。


## 现代商业硬件

专有的硬件在如今正在消失，因此了解现在的商业硬件非常重要。
如今，计算机的扩展通常是在规模上，而不是自身上，这意味着
连接大量小型计算机而不是个别非常大和异常快速（和昂贵）的系统更具成本效益。
之所以如此，是因为快速而廉价的网络硬件被广泛使用。
虽然大型专业系统仍然占有一席之地，这些系统仍然提供商业机会，
但整体市场与商品硬件市场相比相形见绌。截至2007年，红帽预计，对于未来产品，
大多数数据中心的“标准模块”将是一台最多四个插槽的计算机，
每个插槽都装满一个四核CPU，在英特尔CPU的情况下，该插槽将是超线程的。
这意味着数据中心的标准系统将有多达64个虚拟处理器。
大型机器将会被支持，但四插槽、四CPU核心目前被认为是最佳组合，
大多数优化都是针对此类机器的。


